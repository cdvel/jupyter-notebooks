{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for ML\n",
    "\n",
    "Chosen features to train your model have a huge impact on performance. Only highly relevant features should be used to train your models. There are automatic feature selection techniques to help in this step:\n",
    "\n",
    "- Univariate Selection\n",
    "- Recursive Feature Elimination\n",
    "- Principle Component Analysis\n",
    "- Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection\n",
    "\n",
    "- Automatic\n",
    "- Selection process of features that contribute the most to the prediction or output of interest\n",
    "- Irrelevant features are harmful, specially to linear algorithms (linear and logistic regression)\n",
    "\n",
    "Benefits\n",
    "\n",
    "* Reduce overfitting\n",
    "\n",
    "Less redundant data, less decisions based on noise\n",
    "\n",
    "* Improves accuracy\n",
    "\n",
    "Less misleading data, better accuracy\n",
    "\n",
    "* Reduce training time\n",
    "\n",
    "Less data, training is faster\n",
    "\n",
    "More info: http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Univariate feature selection\n",
    "\n",
    "Use (subset) statistical tests to select features strongly related to the output variable. For example, chi-squared for non-negative features to select 4 of the best features (example below)\n",
    "\n",
    "\n",
    "Using scikit-learn's `SelectKBest` class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
      "[  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393\n",
      "   181.304]\n",
      "[[ 148.     0.    33.6   50. ]\n",
      " [  85.     0.    26.6   31. ]\n",
      " [ 183.     0.    23.3   32. ]\n",
      " [  89.    94.    28.1   21. ]\n",
      " [ 137.   168.    43.1   33. ]]\n",
      "\n",
      "Features x Score: \n",
      "[['preg' '111.519690636']\n",
      " ['plas' '1411.88704064']\n",
      " ['pres' '17.6053732153']\n",
      " ['skin' '53.1080398363']\n",
      " ['test' '2175.56527292']\n",
      " ['mass' '127.669343331']\n",
      " ['pedi' '5.39268154697']\n",
      " ['age' '181.303689044']]\n",
      "\n",
      "Top 4: plas, test, mass, age\n"
     ]
    }
   ],
   "source": [
    "#feature extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions, column_stack, asarray\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "print names\n",
    "# Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "#Summarize scores\n",
    "set_printoptions(precision=3)\n",
    "fscore = fit.scores_\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "#Summarize selected features\n",
    "print features[0:5,:]\n",
    "\n",
    "print '\\nFeatures x Score: \\n', \n",
    "\n",
    "print column_stack((names[:-1], fscore2.tolist()));\n",
    "\n",
    "print '\\nTop 4: plas, test, mass, age', \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Recursive Feature Elimination (RFE)\n",
    "\n",
    "Works recursively by: \n",
    "\n",
    "1. Removing attributes, \n",
    "2. Bulding models with remaining attributes, \n",
    "3. Evaluating accuracy, \n",
    "4. Selecting model with best attribute combination \n",
    "\n",
    "\n",
    "[Feature ranking with recursive feature elimination](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    ">Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "Below logistic regression algorithm (any works) to select the top 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 3\n",
      "Selected features [ True False False False False  True  True False]\n",
      "Feature ranking [1 2 3 5 6 1 1 4]\n",
      "Top features by name: preg mass pedi\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe =  RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"Num features: %d\") % fit.n_features_\n",
    "print(\"Selected features %s\") % fit.support_\n",
    "print (\"Feature ranking %s\") % fit.ranking_\n",
    "print \"Top features by name:\",\n",
    "for idx in range(len(names)-1):\n",
    "    if (fit.support_[idx]):\n",
    "        print names[idx],\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Principal Component Analysis (PCA)\n",
    "\n",
    "Compress the dataset with linear algebra,a.k.a., data reduction. \n",
    "[PCA](http://scikit-learn.org/stable/modules/decomposition.html)\n",
    "> PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns n components in its fit method, and can be used on new data to project it on these components.\n",
    "\n",
    "\n",
    "PCA allows you to adjust the number  dimensions/components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance:        [ 13439.051    931.546    390.069] \n",
      "Explained Variance Ratio:  [ 0.889  0.062  0.026] \n",
      "Explained Variance CumSum: [ 0.889  0.95   0.976] \n",
      "[[ -2.022e-03   9.781e-02   1.609e-02   6.076e-02   9.931e-01   1.401e-02\n",
      "    5.372e-04  -3.565e-03]\n",
      " [  2.265e-02   9.722e-01   1.419e-01  -5.786e-02  -9.463e-02   4.697e-02\n",
      "    8.168e-04   1.402e-01]\n",
      " [ -2.246e-02   1.434e-01  -9.225e-01  -3.070e-01   2.098e-02  -1.324e-01\n",
      "   -6.400e-04  -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#extract features\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "#summarize components\n",
    "\n",
    "print(\"Explained Variance:        %s \") % fit.explained_variance_\n",
    "print(\"Explained Variance Ratio:  %s \") % fit.explained_variance_ratio_\n",
    "# By choosing k (number of components)=3, we retain 97.6% of the variance \n",
    "\n",
    "print(\"Explained Variance CumSum: %s \") % fit.explained_variance_ratio_.cumsum()\n",
    "print(fit.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance\n",
    "\n",
    "Some algos can be estimators of feature importance (Bagged decision trees: Random Forest Tree, Extra Trees). \n",
    "\n",
    "[ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "> This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "Below a ExtraTreesClassifier for the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['preg' '0.104827680737']\n",
      " ['plas' '0.231328053099']\n",
      " ['pres' '0.106357372243']\n",
      " ['skin' '0.0776213364668']\n",
      " ['test' '0.0672802413506']\n",
      " ['mass' '0.145849365915']\n",
      " ['pedi' '0.120177755314']\n",
      " ['age' '0.146558194875']]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "#feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print column_stack((names[:-1], model.feature_importances_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
